{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891ba868",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import cfscrape\n",
    "from bs4 import BeautifulSoup\n",
    "import feedparser\n",
    "import os\n",
    "import tarfile\n",
    "from pathlib import Path\n",
    "import typing as t\n",
    "import logging\n",
    "import tempfile\n",
    "from datetime import datetime\n",
    "from dateutil.parser import parse\n",
    "from dateutil.tz import gettz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9223c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s:%(message)s', level=logging.DEBUG, datefmt='%I:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8e6b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "RSS_FILE_URL = 'https://www.livemint.com/rss/news'\n",
    "\n",
    "PARSED_ARTICLES_DIR = os.path.join('data', 'parse_livemint')\n",
    "os.makedirs(PARSED_ARTICLES_DIR, exist_ok=True)\n",
    "\n",
    "ALREADY_LOADED_ARTICLES_DB_FILE = os.path.join(PARSED_ARTICLES_DIR, 'list_of_articles.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca9dd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_links_from_rss(rss_url):\n",
    "    return [entry.link for entry in feedparser.parse(rss_url).entries]\n",
    "\n",
    "class ArticleData(t.NamedTuple):\n",
    "    slug: str\n",
    "    dt: datetime\n",
    "    title: str\n",
    "    text: str\n",
    "    keywords: list\n",
    "    picture_href: str\n",
    "    picture_bytes: bytes\n",
    "    href: str\n",
    "\n",
    "def check_element_exist(f):\n",
    "    def func(*args, **kwargs):\n",
    "        try:\n",
    "            result = f(*args, **kwargs)\n",
    "            return result\n",
    "        except AttributeError:\n",
    "            logger.error(f'Article {args[1]} has no searched element')\n",
    "            result = ''\n",
    "            return result\n",
    "\n",
    "    return func\n",
    "\n",
    "\n",
    "@check_element_exist\n",
    "def get_title(soup, article_link):\n",
    "    return soup.find('h1').text\n",
    "\n",
    "\n",
    "@check_element_exist\n",
    "def get_text(soup, article_link):\n",
    "    return ' '.join([i.text.strip() for i in soup.find('div', class_='mainArea').findAll('p')])\n",
    "\n",
    "\n",
    "@check_element_exist\n",
    "def get_picture_href(soup, article_link):\n",
    "    return soup.find('span', class_='pos-rel dblock imgmobalignment').find('img').get('src')\n",
    "\n",
    "\n",
    "@check_element_exist\n",
    "def get_date(soup, article_link):\n",
    "    return parse(soup.find('meta', attrs={'property': 'article:published_time'}).get('content'))\n",
    "\n",
    "\n",
    "@check_element_exist\n",
    "def get_keywords(soup, article_link):\n",
    "    return soup.find('meta', attrs={'name': 'keywords'}).get('content')\n",
    "\n",
    "\n",
    "def get_data(scrapper, article_link, soup: BeautifulSoup) -> ArticleData:\n",
    "    article_title = get_title(soup, article_link)\n",
    "\n",
    "    article_text = get_text(soup, article_link)\n",
    "\n",
    "    article_pic_href = get_picture_href(soup, article_link)\n",
    "\n",
    "    article_date = get_date(soup, article_link)\n",
    "\n",
    "    article_keywords = get_keywords(soup, article_link)\n",
    "\n",
    "    return ArticleData(\n",
    "        slug=article_link[article_link.rfind(str('/')) + 1:article_link.rfind('.')],\n",
    "        title=article_title,\n",
    "        dt=article_date,\n",
    "        text=article_text,\n",
    "        keywords=article_keywords.split(','),\n",
    "        picture_href=article_pic_href,\n",
    "        picture_bytes=scrapper.get(article_pic_href).content if article_pic_href else '',\n",
    "        href=article_link,\n",
    "    )\n",
    "\n",
    "\n",
    "def archive_files(archive_path, archive_file_name, files):\n",
    "    archive_name = os.path.join(archive_path, f'{archive_file_name}.tar.xz')\n",
    "    with tarfile.open(archive_name, 'w:xz') as tar_obj:\n",
    "        for file in files:\n",
    "            tar_obj.add(file)\n",
    "    logger.info(f'saved to \"{archive_name}\"')\n",
    "\n",
    "\n",
    "def save_cached_articles(links: t.Set[str]) -> None:\n",
    "    with open(ALREADY_LOADED_ARTICLES_DB_FILE, 'wt', encoding='utf-8') as file:\n",
    "        file.writelines(f'{i}\\n' for i in sorted(links))\n",
    "\n",
    "\n",
    "def load_cached_articles() -> t.Set[str]:\n",
    "    if not os.path.exists(ALREADY_LOADED_ARTICLES_DB_FILE):\n",
    "        return set()\n",
    "\n",
    "    with open(ALREADY_LOADED_ARTICLES_DB_FILE, 'rt', encoding='utf-8') as file:\n",
    "        return set(line.strip() for line in file.readlines())\n",
    "\n",
    "def parsing(rss_file_url) -> None:\n",
    "    all_article_links = get_article_links_from_rss(rss_file_url)\n",
    "    old_article_links = load_cached_articles()\n",
    "    new_article_links = set(filter(lambda x: x not in old_article_links, all_article_links))\n",
    "    successfully_saved_links = set()\n",
    "\n",
    "    scrapper = cfscrape.create_scraper()\n",
    "    logger.info(f'Count articles for parsing: {len(new_article_links)}')\n",
    "\n",
    "    try:\n",
    "        for article_link in new_article_links:\n",
    "            logger.info(f'Process article \"{article_link}\"')\n",
    "\n",
    "            article_response = scrapper.get(article_link)\n",
    "            try:\n",
    "                article_response.raise_for_status()\n",
    "            except Exception as e:\n",
    "                logger.error(e)\n",
    "                continue\n",
    "\n",
    "            soup = BeautifulSoup(article_response.text, 'lxml')\n",
    "            info = get_data(scrapper, article_link, soup)\n",
    "\n",
    "            files_to_archive = []\n",
    "            with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "                with (\n",
    "                    open(os.path.join(tmpdirname, 'article.html'), 'w', encoding='utf-8') as html_file,\n",
    "                    open(os.path.join(tmpdirname, 'data.json'), 'w+t') as json_file,\n",
    "                ):\n",
    "                    json.dump({\n",
    "                        'title': info.title,\n",
    "                        'text': info.text,\n",
    "                        'publication_dt': info.dt.isoformat(' ', \"seconds\"),\n",
    "                        'parsing_dt': datetime.now(gettz()).isoformat(' ', 'seconds'),\n",
    "                        'meta_keywords': info.keywords,\n",
    "                        'href': info.href,\n",
    "                    }, json_file, indent=4)\n",
    "                    html_file.write(article_response.text)\n",
    "                    files_to_archive.append(html_file.name)\n",
    "\n",
    "                    json_file.seek(0)\n",
    "                    files_to_archive.append(json_file.name)\n",
    "\n",
    "                    if info.picture_bytes:\n",
    "                        pic_format = info.picture_href[info.picture_href.rfind('.') + 1:]\n",
    "                        with open(os.path.join(tmpdirname, f'header_pic.{pic_format}'), 'wb') as img_file:\n",
    "                            img_file.write(info.picture_bytes)\n",
    "                            files_to_archive.append(img_file.name)\n",
    "\n",
    "                    archive_files(PARSED_ARTICLES_DIR, info.slug, files_to_archive)\n",
    "                    successfully_saved_links.add(article_link)\n",
    "\n",
    "        save_cached_articles(old_article_links.union(successfully_saved_links))\n",
    "        logger.info(f'Count articles for saving: {len(successfully_saved_links)}')\n",
    "    except KeyboardInterrupt:\n",
    "        save_cached_articles(old_article_links.union(successfully_saved_links))\n",
    "        logger.info(f'Keyboard interrupt.Count articles for saving: {len(successfully_saved_links)}')\n",
    "\n",
    "parsing(RSS_FILE_URL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
