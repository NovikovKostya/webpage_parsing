{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dffbe59a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T20:55:54.056507418Z",
     "start_time": "2023-06-01T20:55:54.012680679Z"
    }
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import tarfile\n",
    "import tempfile\n",
    "import typing as t\n",
    "from datetime import datetime\n",
    "\n",
    "import cfscrape\n",
    "import feedparser\n",
    "import pytz\n",
    "from bs4 import BeautifulSoup\n",
    "from dateutil.parser import parse\n",
    "from dateutil.tz import gettz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5422c6e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T20:55:54.385745456Z",
     "start_time": "2023-06-01T20:55:54.378899986Z"
    }
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s:%(message)s', level=logging.DEBUG, datefmt='%I:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d3af258",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T20:55:54.857337396Z",
     "start_time": "2023-06-01T20:55:54.855484606Z"
    }
   },
   "outputs": [],
   "source": [
    "RSS_FILE_URL = 'https://www.investing.com/rss/news_301.rss'\n",
    "\n",
    "PARSED_ARTICLES_DIR = os.path.join('data', 'parse_investing')\n",
    "os.makedirs(PARSED_ARTICLES_DIR, exist_ok=True)\n",
    "\n",
    "ALREADY_LOADED_ARTICLES_DB_FILE = os.path.join(PARSED_ARTICLES_DIR, 'list_of_articles.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02539fa7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T20:55:55.373426945Z",
     "start_time": "2023-06-01T20:55:55.241579872Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:55:55 INFO:Count articles for parsing: 0\n",
      "11:55:55 INFO:Count articles for saving: 0\n"
     ]
    }
   ],
   "source": [
    "def get_article_links_from_rss(rss_url):\n",
    "    return [entry.link for entry in feedparser.parse(rss_url).entries]\n",
    "\n",
    "\n",
    "class ArticleData(t.NamedTuple):\n",
    "    slug: str\n",
    "    dt: datetime\n",
    "    title: str\n",
    "    text: str\n",
    "    language: str\n",
    "    picture_href: str\n",
    "    picture_bytes: bytes\n",
    "    href: str\n",
    "\n",
    "def remove_tag(*args: t.List) -> None:\n",
    "    for arg in args:\n",
    "        for tag in arg:\n",
    "            tag.decompose()\n",
    "\n",
    "def check_element_exist(f):\n",
    "    @functools.wraps\n",
    "    def func(*args, **kwargs):\n",
    "        try:\n",
    "            result = f(*args, **kwargs)\n",
    "            return result\n",
    "        except AttributeError:\n",
    "            logger.error(f'Article {args[1]} has no searched element')\n",
    "            result = ''\n",
    "            return result\n",
    "\n",
    "    return func\n",
    "\n",
    "\n",
    "@check_element_exist\n",
    "def get_title(soup, article_link):\n",
    "    return soup.find('h1').text\n",
    "\n",
    "\n",
    "@check_element_exist\n",
    "def get_text(soup, article_link):\n",
    "    return ''.join(soup.find('div', class_='WYSIWYG articlePage').findAll(string=True)).strip()\n",
    "\n",
    "\n",
    "@check_element_exist\n",
    "def get_picture_href(soup, article_link):\n",
    "    return soup.find('div', class_='WYSIWYG articlePage').find('img').get('src')\n",
    "\n",
    "\n",
    "@check_element_exist\n",
    "def get_language(soup, article_link):\n",
    "    return soup.find('meta', attrs={'http-equiv': 'content-language'}).get('content')\n",
    "\n",
    "\n",
    "def get_data(scrapper, article_link, soup: BeautifulSoup) -> ArticleData:\n",
    "    article_title = get_title(soup, article_link)\n",
    "\n",
    "    remove_tag(soup.find('div', class_='WYSIWYG articlePage').findAll('script'),\n",
    "               soup.findAll('div', class_='relatedInstrumentsWrapper'),\n",
    "               soup.find('div', class_='imgCarousel').findAll('span'))\n",
    "\n",
    "    article_text = get_text(soup, article_link)\n",
    "\n",
    "    article_pic_href = get_picture_href(soup, article_link)\n",
    "\n",
    "    article_language = get_language(soup, article_link)\n",
    "\n",
    "    dates = {}\n",
    "    if soup.select('div.contentSectionDetails span'):\n",
    "        for i in soup.select('div.contentSectionDetails span'):\n",
    "            d_s = i.text.split(' ')\n",
    "            dates[d_s[0].strip().lower()] = parse(' '.join(d_s[1:-1])).replace(tzinfo=pytz.timezone('EST'))\n",
    "    else:\n",
    "        logger.error(f'Article {article_link} has no searched element')\n",
    "\n",
    "    return ArticleData(\n",
    "        slug=article_link[article_link.rfind(str('/')) + 1:],\n",
    "        title=article_title,\n",
    "        dt=dates['published'] if dates else '',\n",
    "        text=article_text,\n",
    "        language= article_language,\n",
    "        picture_href= article_pic_href,\n",
    "        picture_bytes=scrapper.get(article_pic_href).content if article_pic_href else '',\n",
    "        href=article_link,\n",
    "    )\n",
    "\n",
    "\n",
    "def archive_files(archive_path, archive_file_name, files):\n",
    "    archive_name = os.path.join(archive_path, f'{archive_file_name}.tar.xz')\n",
    "    with tarfile.open(archive_name, 'w:xz') as tar_obj:\n",
    "        for file in files:\n",
    "            tar_obj.add(file)\n",
    "    logger.info(f'Saved to \"{archive_name}\"')\n",
    "\n",
    "\n",
    "def save_cached_articles(links: t.Set[str]) -> None:\n",
    "    with open(ALREADY_LOADED_ARTICLES_DB_FILE, 'wt', encoding='utf-8') as file:\n",
    "        file.writelines(f'{i}\\n' for i in sorted(links))\n",
    "\n",
    "\n",
    "def load_cached_articles() -> t.Set[str]:\n",
    "    if not os.path.exists(ALREADY_LOADED_ARTICLES_DB_FILE):\n",
    "        return set()\n",
    "\n",
    "    with open(ALREADY_LOADED_ARTICLES_DB_FILE, 'rt', encoding='utf-8') as file:\n",
    "        return set(line.strip() for line in file.readlines())\n",
    "\n",
    "\n",
    "def parsing(rss_file_url) -> None:\n",
    "    all_article_links = get_article_links_from_rss(rss_file_url)\n",
    "    old_article_links = load_cached_articles()\n",
    "    new_article_links = set(filter(lambda x: x not in old_article_links, all_article_links))\n",
    "    successfully_saved_links = set()\n",
    "\n",
    "    scrapper = cfscrape.create_scraper()\n",
    "    logger.info(f'Count articles for parsing: {len(new_article_links)}')\n",
    "\n",
    "    try:\n",
    "        for article_link in new_article_links:\n",
    "            logger.info(f'Process article \"{article_link}\"')\n",
    "\n",
    "            article_response = scrapper.get(article_link)\n",
    "            try:\n",
    "                article_response.raise_for_status()\n",
    "            except Exception as e:\n",
    "                logger.error(e)\n",
    "                continue\n",
    "\n",
    "            soup = BeautifulSoup(article_response.text, 'lxml')\n",
    "            info = get_data(scrapper, article_link, soup)\n",
    "\n",
    "            files_to_archive = []\n",
    "            with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "                with (\n",
    "                    open(os.path.join(tmpdirname, 'article.html'), 'w', encoding='utf-8') as html_file,\n",
    "                    open(os.path.join(tmpdirname, 'data.json'), 'w+t') as json_file,\n",
    "                ):\n",
    "                    json.dump({\n",
    "                        'title': info.title,\n",
    "                        'text': info.text,\n",
    "                        'publication_dt': info.dt.isoformat(' ', \"seconds\"),\n",
    "                        'parsing_dt': datetime.now(gettz()).isoformat(' ', 'seconds'),\n",
    "                        'language': info.language,\n",
    "                        'href': info.href,\n",
    "                    }, json_file, indent=4)\n",
    "                    html_file.write(article_response.text)\n",
    "                    files_to_archive.append(html_file.name)\n",
    "\n",
    "                    json_file.seek(0)\n",
    "                    files_to_archive.append(json_file.name)\n",
    "\n",
    "                    if info.picture_bytes:\n",
    "                        pic_format = info.picture_href[info.picture_href.rfind('.') + 1:]\n",
    "                        with open(os.path.join(tmpdirname, f'header_pic.{pic_format}'), 'wb') as img_file:\n",
    "                            img_file.write(info.picture_bytes)\n",
    "                            files_to_archive.append(img_file.name)\n",
    "\n",
    "                    archive_files(PARSED_ARTICLES_DIR, info.slug, files_to_archive)\n",
    "                    successfully_saved_links.add(article_link)\n",
    "\n",
    "        save_cached_articles(old_article_links.union(successfully_saved_links))\n",
    "        logger.info(f'Count articles for saving: {len(successfully_saved_links)}')\n",
    "    except KeyboardInterrupt:\n",
    "        save_cached_articles(old_article_links.union(successfully_saved_links))\n",
    "        logger.info(f'Keyboard interrupt.Count articles for saving: {len(successfully_saved_links)}')\n",
    "\n",
    "parsing(RSS_FILE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76363edd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
