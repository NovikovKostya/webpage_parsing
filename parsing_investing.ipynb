{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dffbe59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import cfscrape\n",
    "from bs4 import BeautifulSoup\n",
    "import feedparser\n",
    "import os\n",
    "import tarfile\n",
    "from pathlib import Path\n",
    "import typing as t\n",
    "import logging\n",
    "import tempfile\n",
    "from datetime import datetime\n",
    "from dateutil.parser import parse\n",
    "from dateutil.tz import gettz\n",
    "import pytz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5422c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s:%(message)s', level=logging.DEBUG, datefmt='%I:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d3af258",
   "metadata": {},
   "outputs": [],
   "source": [
    "RSS_FILE_URL = 'https://www.investing.com/rss/news_301.rss'\n",
    "\n",
    "PARSED_ARTICLES_DIR = os.path.join('data', 'parse_investing')\n",
    "os.makedirs(PARSED_ARTICLES_DIR, exist_ok=True)\n",
    "\n",
    "ALREADY_LOADED_ARTICLES_DB_FILE = os.path.join(PARSED_ARTICLES_DIR, 'list_of_articles.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02539fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01:33:38 INFO:Count articles for parsing: 10\n",
      "01:33:38 INFO:Process article \"https://www.investing.com/news/cryptocurrency-news/hong-kongs-sfc-releases-circular-on-vatps-new-licensing-system-3095830\"\n",
      "01:33:38 DEBUG:Starting new HTTPS connection (1): www.investing.com:443\n",
      "01:33:39 DEBUG:https://www.investing.com:443 \"GET /news/cryptocurrency-news/hong-kongs-sfc-releases-circular-on-vatps-new-licensing-system-3095830 HTTP/1.1\" 200 None\n",
      "01:33:39 DEBUG:Starting new HTTPS connection (1): i-invdn-com.investing.com:443\n",
      "01:33:39 DEBUG:https://i-invdn-com.investing.com:443 \"GET /news/LYNXNPEE1J1IK_L.jpg HTTP/1.1\" 200 129680\n",
      "01:33:39 INFO:Saved to \"data\\parse_investing\\hong-kongs-sfc-releases-circular-on-vatps-new-licensing-system-3095830.tar.xz\"\n",
      "01:33:39 INFO:Process article \"https://www.investing.com/news/cryptocurrency-news/winklevoss-twins-gemini-exchange-to-seek-crypto-license-in-uae-3095796\"\n",
      "01:33:39 DEBUG:https://www.investing.com:443 \"GET /news/cryptocurrency-news/winklevoss-twins-gemini-exchange-to-seek-crypto-license-in-uae-3095796 HTTP/1.1\" 200 None\n",
      "01:33:39 DEBUG:Starting new HTTPS connection (1): d1-invdn-com.investing.com:443\n",
      "01:33:40 DEBUG:https://d1-invdn-com.investing.com:443 \"GET /content/picdb2fc1fffbbbfcffb98527a684a0d918.jpg HTTP/1.1\" 200 20929\n",
      "01:33:40 INFO:Saved to \"data\\parse_investing\\winklevoss-twins-gemini-exchange-to-seek-crypto-license-in-uae-3095796.tar.xz\"\n",
      "01:33:40 INFO:Process article \"https://www.investing.com/news/cryptocurrency-news/germanys-deutsche-telekom-plugs-in-as-polygon-validator-3095974\"\n",
      "01:33:40 DEBUG:https://www.investing.com:443 \"GET /news/cryptocurrency-news/germanys-deutsche-telekom-plugs-in-as-polygon-validator-3095974 HTTP/1.1\" 200 None\n",
      "01:33:40 DEBUG:https://i-invdn-com.investing.com:443 \"GET /news/Cryptocurrencies_800x533_L_1535981327.jpg HTTP/1.1\" 200 66029\n",
      "01:33:40 INFO:Saved to \"data\\parse_investing\\germanys-deutsche-telekom-plugs-in-as-polygon-validator-3095974.tar.xz\"\n",
      "01:33:40 INFO:Process article \"https://www.investing.com/news/cryptocurrency-news/tron-blockchain-sets-new-record-with-109-m-daily-transactions-3095888\"\n",
      "01:33:40 DEBUG:https://www.investing.com:443 \"GET /news/cryptocurrency-news/tron-blockchain-sets-new-record-with-109-m-daily-transactions-3095888 HTTP/1.1\" 200 None\n",
      "01:33:40 DEBUG:https://i-invdn-com.investing.com:443 \"GET /news/Cryptocurrencies_800x533_L_1556444856.jpg HTTP/1.1\" 200 142735\n",
      "01:33:41 INFO:Saved to \"data\\parse_investing\\tron-blockchain-sets-new-record-with-109-m-daily-transactions-3095888.tar.xz\"\n",
      "01:33:41 INFO:Process article \"https://www.investing.com/news/cryptocurrency-news/nifty-news-pixel-penguin-accused-of-charity-rug-epic-adds-new-nft-games-and-more-3095847\"\n",
      "01:33:41 DEBUG:https://www.investing.com:443 \"GET /news/cryptocurrency-news/nifty-news-pixel-penguin-accused-of-charity-rug-epic-adds-new-nft-games-and-more-3095847 HTTP/1.1\" 200 None\n",
      "01:33:41 DEBUG:https://i-invdn-com.investing.com:443 \"GET /news/Cryptocurrencies_800x533_L_1556444856.jpg HTTP/1.1\" 200 142735\n",
      "01:33:41 INFO:Saved to \"data\\parse_investing\\nifty-news-pixel-penguin-accused-of-charity-rug-epic-adds-new-nft-games-and-more-3095847.tar.xz\"\n",
      "01:33:41 INFO:Process article \"https://www.investing.com/news/cryptocurrency-news/binance-plans-new-round-of-layoffs-amid-increased-regulatory-scrutiny-3095893\"\n",
      "01:33:41 DEBUG:https://www.investing.com:443 \"GET /news/cryptocurrency-news/binance-plans-new-round-of-layoffs-amid-increased-regulatory-scrutiny-3095893 HTTP/1.1\" 200 None\n",
      "01:33:41 DEBUG:https://i-invdn-com.investing.com:443 \"GET /news/Cryptocurrencies_800x533_L_1556444946.jpg HTTP/1.1\" 200 140606\n",
      "01:33:41 INFO:Saved to \"data\\parse_investing\\binance-plans-new-round-of-layoffs-amid-increased-regulatory-scrutiny-3095893.tar.xz\"\n",
      "01:33:41 INFO:Process article \"https://www.investing.com/news/cryptocurrency-news/rumors-on-binance-and-gateio-platforms-clarify-the-issues-3095973\"\n",
      "01:33:42 DEBUG:https://www.investing.com:443 \"GET /news/cryptocurrency-news/rumors-on-binance-and-gateio-platforms-clarify-the-issues-3095973 HTTP/1.1\" 200 None\n",
      "01:33:42 DEBUG:https://i-invdn-com.investing.com:443 \"GET /news/Cryptocurrencies_800x533_L_1535981327.jpg HTTP/1.1\" 200 66029\n",
      "01:33:42 INFO:Saved to \"data\\parse_investing\\rumors-on-binance-and-gateio-platforms-clarify-the-issues-3095973.tar.xz\"\n",
      "01:33:42 INFO:Process article \"https://www.investing.com/news/cryptocurrency-news/australia-asks-if-highrisk-ai-should-be-banned-in-surprise-consultation-3095842\"\n",
      "01:33:42 DEBUG:https://www.investing.com:443 \"GET /news/cryptocurrency-news/australia-asks-if-highrisk-ai-should-be-banned-in-surprise-consultation-3095842 HTTP/1.1\" 200 None\n",
      "01:33:42 DEBUG:https://i-invdn-com.investing.com:443 \"GET /news/LYNXNPEE1J1IK_L.jpg HTTP/1.1\" 200 129680\n",
      "01:33:42 INFO:Saved to \"data\\parse_investing\\australia-asks-if-highrisk-ai-should-be-banned-in-surprise-consultation-3095842.tar.xz\"\n",
      "01:33:42 INFO:Process article \"https://www.investing.com/news/cryptocurrency-news/japanese-ai-experts-raise-concern-over-bots-trained-on-copyrighted-material-3095794\"\n",
      "01:33:43 DEBUG:https://www.investing.com:443 \"GET /news/cryptocurrency-news/japanese-ai-experts-raise-concern-over-bots-trained-on-copyrighted-material-3095794 HTTP/1.1\" 200 None\n",
      "01:33:43 DEBUG:https://i-invdn-com.investing.com:443 \"GET /news/Cryptocurrencies_800x533_L_1556528163.jpg HTTP/1.1\" 200 143540\n",
      "01:33:43 INFO:Saved to \"data\\parse_investing\\japanese-ai-experts-raise-concern-over-bots-trained-on-copyrighted-material-3095794.tar.xz\"\n",
      "01:33:43 INFO:Process article \"https://www.investing.com/news/cryptocurrency-news/missing-bitcoin-millionaire-and-onfo-coin-cocreator-found-dead-report-3095880\"\n",
      "01:33:43 DEBUG:https://www.investing.com:443 \"GET /news/cryptocurrency-news/missing-bitcoin-millionaire-and-onfo-coin-cocreator-found-dead-report-3095880 HTTP/1.1\" 200 None\n",
      "01:33:43 DEBUG:https://i-invdn-com.investing.com:443 \"GET /news/LYNXNPEE1P15Z_L.jpg HTTP/1.1\" 200 75786\n",
      "01:33:43 INFO:Saved to \"data\\parse_investing\\missing-bitcoin-millionaire-and-onfo-coin-cocreator-found-dead-report-3095880.tar.xz\"\n",
      "01:33:43 INFO:Count articles for saving: 10\n"
     ]
    }
   ],
   "source": [
    "def get_article_links_from_rss(rss_url):\n",
    "    return [entry.link for entry in feedparser.parse(rss_url).entries]\n",
    "\n",
    "\n",
    "class ArticleData(t.NamedTuple):\n",
    "    slug: str\n",
    "    dt: datetime\n",
    "    title: str\n",
    "    text: str\n",
    "    language: str\n",
    "    picture_href: str\n",
    "    picture_bytes: bytes\n",
    "    href: str\n",
    "\n",
    "def remove_tag(*args: t.List) -> None:\n",
    "    for arg in args:\n",
    "        for tag in arg:\n",
    "            tag.decompose()\n",
    "\n",
    "def check_element_exist(f):\n",
    "    def func(*args, **kwargs):\n",
    "        try:\n",
    "            result = f(*args, **kwargs)\n",
    "            return result\n",
    "        except AttributeError:\n",
    "            logger.error(f'Article {args[1]} has no searched element')\n",
    "            result = ''\n",
    "            return result\n",
    "\n",
    "    return func\n",
    "\n",
    "\n",
    "@check_element_exist\n",
    "def get_title(soup, article_link):\n",
    "    return soup.find('h1').text\n",
    "\n",
    "\n",
    "@check_element_exist\n",
    "def get_text(soup, article_link):\n",
    "    return ''.join(soup.find('div', class_='WYSIWYG articlePage').findAll(string=True)).strip()\n",
    "\n",
    "\n",
    "@check_element_exist\n",
    "def get_picture_href(soup, article_link):\n",
    "    return soup.find('div', class_='WYSIWYG articlePage').find('img').get('src')\n",
    "\n",
    "\n",
    "@check_element_exist\n",
    "def get_language(soup, article_link):\n",
    "    return soup.find('meta', attrs={'http-equiv': 'content-language'}).get('content')\n",
    "\n",
    "\n",
    "def get_data(scrapper, article_link, soup: BeautifulSoup) -> ArticleData:\n",
    "    article_title = get_title(soup, article_link)\n",
    "\n",
    "    remove_tag(soup.find('div', class_='WYSIWYG articlePage').findAll('script'),\n",
    "               soup.findAll('div', class_='relatedInstrumentsWrapper'),\n",
    "               soup.find('div', class_='imgCarousel').findAll('span'))\n",
    "\n",
    "    article_text = get_text(soup, article_link)\n",
    "\n",
    "    article_pic_href = get_picture_href(soup, article_link)\n",
    "\n",
    "    article_language = get_language(soup, article_link)\n",
    "\n",
    "    dates = {}\n",
    "    if soup.select('div.contentSectionDetails span'):\n",
    "        for i in soup.select('div.contentSectionDetails span'):\n",
    "            d_s = i.text.split(' ')\n",
    "            dates[d_s[0].strip().lower()] = parse(' '.join(d_s[1:-1])).replace(tzinfo=pytz.timezone('EST'))\n",
    "    else:\n",
    "        logger.error(f'Article {article_link} has no searched element')\n",
    "\n",
    "    return ArticleData(\n",
    "        slug=article_link[article_link.rfind(str('/')) + 1:],\n",
    "        title=article_title,\n",
    "        dt=dates['published'] if dates else '',\n",
    "        text=article_text,\n",
    "        language= article_language,\n",
    "        picture_href= article_pic_href,\n",
    "        picture_bytes=scrapper.get(article_pic_href).content if article_pic_href else '',\n",
    "        href=article_link,\n",
    "    )\n",
    "\n",
    "\n",
    "def archive_files(archive_path, archive_file_name, files):\n",
    "    archive_name = os.path.join(archive_path, f'{archive_file_name}.tar.xz')\n",
    "    with tarfile.open(archive_name, 'w:xz') as tar_obj:\n",
    "        for file in files:\n",
    "            tar_obj.add(file)\n",
    "    logger.info(f'Saved to \"{archive_name}\"')\n",
    "\n",
    "\n",
    "def save_cached_articles(links: t.Set[str]) -> None:\n",
    "    with open(ALREADY_LOADED_ARTICLES_DB_FILE, 'wt', encoding='utf-8') as file:\n",
    "        file.writelines(f'{i}\\n' for i in sorted(links))\n",
    "\n",
    "\n",
    "def load_cached_articles() -> t.Set[str]:\n",
    "    if not os.path.exists(ALREADY_LOADED_ARTICLES_DB_FILE):\n",
    "        return set()\n",
    "\n",
    "    with open(ALREADY_LOADED_ARTICLES_DB_FILE, 'rt', encoding='utf-8') as file:\n",
    "        return set(line.strip() for line in file.readlines())\n",
    "\n",
    "\n",
    "def parsing(rss_file_url) -> None:\n",
    "    all_article_links = get_article_links_from_rss(rss_file_url)\n",
    "    old_article_links = load_cached_articles()\n",
    "    new_article_links = set(filter(lambda x: x not in old_article_links, all_article_links))\n",
    "    successfully_saved_links = set()\n",
    "\n",
    "    scrapper = cfscrape.create_scraper()\n",
    "    logger.info(f'Count articles for parsing: {len(new_article_links)}')\n",
    "\n",
    "    try:\n",
    "        for article_link in new_article_links:\n",
    "            logger.info(f'Process article \"{article_link}\"')\n",
    "\n",
    "            article_response = scrapper.get(article_link)\n",
    "            try:\n",
    "                article_response.raise_for_status()\n",
    "            except Exception as e:\n",
    "                logger.error(e)\n",
    "                continue\n",
    "\n",
    "            soup = BeautifulSoup(article_response.text, 'lxml')\n",
    "            info = get_data(scrapper, article_link, soup)\n",
    "\n",
    "            files_to_archive = []\n",
    "            with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "                with (\n",
    "                    open(os.path.join(tmpdirname, 'article.html'), 'w', encoding='utf-8') as html_file,\n",
    "                    open(os.path.join(tmpdirname, 'data.json'), 'w+t') as json_file,\n",
    "                ):\n",
    "                    json.dump({\n",
    "                        'title': info.title,\n",
    "                        'text': info.text,\n",
    "                        'publication_dt': info.dt.isoformat(' ', \"seconds\"),\n",
    "                        'parsing_dt': datetime.now(gettz()).isoformat(' ', 'seconds'),\n",
    "                        'language': info.language,\n",
    "                        'href': info.href,\n",
    "                    }, json_file, indent=4)\n",
    "                    html_file.write(article_response.text)\n",
    "                    files_to_archive.append(html_file.name)\n",
    "\n",
    "                    json_file.seek(0)\n",
    "                    files_to_archive.append(json_file.name)\n",
    "\n",
    "                    if info.picture_bytes:\n",
    "                        pic_format = info.picture_href[info.picture_href.rfind('.') + 1:]\n",
    "                        with open(os.path.join(tmpdirname, f'header_pic.{pic_format}'), 'wb') as img_file:\n",
    "                            img_file.write(info.picture_bytes)\n",
    "                            files_to_archive.append(img_file.name)\n",
    "\n",
    "                    archive_files(PARSED_ARTICLES_DIR, info.slug, files_to_archive)\n",
    "                    successfully_saved_links.add(article_link)\n",
    "\n",
    "        save_cached_articles(old_article_links.union(successfully_saved_links))\n",
    "        logger.info(f'Count articles for saving: {len(successfully_saved_links)}')\n",
    "    except KeyboardInterrupt:\n",
    "        save_cached_articles(old_article_links.union(successfully_saved_links))\n",
    "        logger.info(f'Keyboard interrupt.Count articles for saving: {len(successfully_saved_links)}')\n",
    "\n",
    "parsing(RSS_FILE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76363edd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
