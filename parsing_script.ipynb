{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dffbe59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import cfscrape\n",
    "from bs4 import BeautifulSoup\n",
    "import feedparser\n",
    "import os\n",
    "import tarfile\n",
    "from pathlib import Path\n",
    "import typing as t\n",
    "import logging\n",
    "import tempfile\n",
    "from datetime import datetime\n",
    "from dateutil.parser import parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5422c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger('scrapper').setLevel(logging.DEBUG)\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s:%(message)s', level=logging.DEBUG, datefmt='%I:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d3af258",
   "metadata": {},
   "outputs": [],
   "source": [
    "RSS_FILE_URL = 'https://www.investing.com/rss/news_301.rss'\n",
    "\n",
    "PARSED_ARTICLES_DIR = os.path.join('data', 'parse_kostya_investing')\n",
    "os.makedirs(PARSED_ARTICLES_DIR, exist_ok=True)\n",
    "\n",
    "ALREADY_LOADED_ARTICLES_DB_FILE = os.path.join(PARSED_ARTICLES_DIR, 'list_of_articles.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02539fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01:57:24 INFO:Count articles for parsing: 0\n",
      "01:57:24 INFO:Count articles for saving: 0\n"
     ]
    }
   ],
   "source": [
    "def get_article_links_from_rss(rss_url):\n",
    "    return [entry.link for entry in feedparser.parse(rss_url).entries]\n",
    "\n",
    "\n",
    "class ArticleData(t.NamedTuple):\n",
    "    slug: str\n",
    "    dt: datetime\n",
    "    title: str\n",
    "    text: str\n",
    "    picture_bytes: bytes\n",
    "    href: str\n",
    "\n",
    "\n",
    "def get_data(scrapper, article_link, soup: BeautifulSoup) -> ArticleData:\n",
    "    article_title = soup.find('h1').text\n",
    "    for script in soup.find('div', class_=\"WYSIWYG articlePage\").find_all('script'):\n",
    "        script.decompose()\n",
    "    for div in soup.findAll('div', class_='relatedInstrumentsWrapper'):\n",
    "        div.decompose()\n",
    "\n",
    "    article_text = ''.join(soup.find('div', class_=\"WYSIWYG articlePage\").findAll(string=True)).strip()\n",
    "\n",
    "    try:\n",
    "        article_pic_href = soup.find('div', class_=\"WYSIWYG articlePage\").find('img').get('src')\n",
    "    except Exception as e:\n",
    "        logger.error(e)\n",
    "        article_pic_href = None\n",
    "\n",
    "    dates = {}\n",
    "    for i in soup.select('div.contentSectionDetails span'):\n",
    "        d_s = i.text.split(' ')\n",
    "        dates[d_s[0].strip().lower()] = parse(' '.join(d_s[1:-1]))\n",
    "\n",
    "    return ArticleData(\n",
    "        slug=article_link[article_link.rfind(str('/')) + 1:],\n",
    "        title=article_title,\n",
    "        dt=dates['published'],\n",
    "        text=article_text,\n",
    "        picture_bytes=scrapper.get(article_pic_href).content if article_pic_href is not None else '',\n",
    "        href=article_link,\n",
    "    )\n",
    "\n",
    "\n",
    "def archive_files(archive_path, archive_file_name, files):\n",
    "    archive_name = os.path.join(archive_path, f'{archive_file_name}.tar.xz')\n",
    "    with tarfile.open(archive_name, 'w:xz') as tar_obj:\n",
    "        for file in files:\n",
    "            tar_obj.add(file)\n",
    "    logger.info(f'saved to \"{archive_name}\"')\n",
    "\n",
    "\n",
    "def save_cached_articles(links: t.Set[str]) -> None:\n",
    "    with open(ALREADY_LOADED_ARTICLES_DB_FILE, 'wt', encoding='utf-8') as file:\n",
    "        file.writelines(f'{i}\\n' for i in sorted(links))\n",
    "\n",
    "\n",
    "def load_cached_articles() -> t.Set[str]:\n",
    "    if not os.path.exists(ALREADY_LOADED_ARTICLES_DB_FILE):\n",
    "        return set()\n",
    "\n",
    "    with open(ALREADY_LOADED_ARTICLES_DB_FILE, 'rt', encoding='utf-8') as file:\n",
    "        return set(line.strip() for line in file.readlines())\n",
    "\n",
    "\n",
    "def parsing(rss_file_url) -> None:\n",
    "    all_article_links = get_article_links_from_rss(rss_file_url)\n",
    "    old_article_links = load_cached_articles()\n",
    "    new_article_links = set(filter(lambda x: x not in old_article_links, all_article_links))\n",
    "    successfully_saved_links = set()\n",
    "\n",
    "    scrapper = cfscrape.create_scraper()\n",
    "    logger.info(f\"Count articles for parsing: {len(new_article_links)}\")\n",
    "\n",
    "    try:\n",
    "        for article_link in new_article_links:\n",
    "            logger.info(f'process article \"{article_link}\"')\n",
    "\n",
    "            article_response = scrapper.get(article_link)\n",
    "            try:\n",
    "                article_response.raise_for_status()\n",
    "            except Exception as e:\n",
    "                logger.error(e)\n",
    "                continue\n",
    "\n",
    "            soup = BeautifulSoup(article_response.text, 'lxml')\n",
    "            info = get_data(scrapper, article_link, soup)\n",
    "\n",
    "            files_to_archive = []\n",
    "            with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "                with (\n",
    "                    open(os.path.join(tmpdirname, 'article.html'), 'w', encoding='utf-8') as html_file,\n",
    "                    open(os.path.join(tmpdirname, 'data.json'), 'w+t') as json_file,\n",
    "                ):\n",
    "                    json.dump({\n",
    "                        'title': info.title,\n",
    "                        'text': info.text,\n",
    "                        'publication_dt': info.dt.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                        'parsing_dt': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                        'locale': soup.find(\"meta\", attrs={\"property\": \"og:locale\"}).get(\"content\"),\n",
    "                        'href': info.href,\n",
    "                        'author': json.loads(soup.find('script', type='application/ld+json').text)['author']['name'],\n",
    "                        'article_section': json.loads(soup.find('script', type='application/ld+json').text)['articleSection'],\n",
    "                    }, json_file, indent=4)\n",
    "                    html_file.write(article_response.text)\n",
    "                    files_to_archive.append(html_file.name)\n",
    "\n",
    "                    json_file.seek(0)\n",
    "                    files_to_archive.append(json_file.name)\n",
    "\n",
    "                    if info.picture_bytes:\n",
    "                        article_pic_href = soup.find('div', class_=\"WYSIWYG articlePage\").find('img').get('src')\n",
    "                        pic_format = article_pic_href[article_pic_href.rfind('.') + 1:]\n",
    "                        with open(os.path.join(tmpdirname, f'header_pic.{pic_format}'), 'wb') as img_file:\n",
    "                            img_file.write(info.picture_bytes)\n",
    "                            files_to_archive.append(img_file.name)\n",
    "\n",
    "                    archive_files(PARSED_ARTICLES_DIR, info.slug, files_to_archive)\n",
    "                    successfully_saved_links.add(article_link)\n",
    "\n",
    "        save_cached_articles(old_article_links.union(successfully_saved_links))\n",
    "        logger.info(f\"Count articles for saving: {len(successfully_saved_links)}\")\n",
    "    except KeyboardInterrupt:\n",
    "        save_cached_articles(old_article_links.union(successfully_saved_links))\n",
    "        logger.info(f\"Keyboard interrupt.Count articles for saving: {len(successfully_saved_links)}\")\n",
    "\n",
    "parsing(RSS_FILE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76363edd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
