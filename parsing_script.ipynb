{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dffbe59a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count articles for parsing 10\n",
      "Proceed article: https://www.investing.com/news/cryptocurrency-news/buying-a-car-with-bitcoin-gets-37m-fine-prison-time-in-morocco-3079992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_4576\\2316048487.py:35: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  article_text = ''.join(soup.find('div', class_=\"WYSIWYG articlePage\").find_all(text=True)).lstrip()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buying-a-car-with-bitcoin-gets-37m-fine-prison-time-in-morocco-3079992\n",
      "C:\\Users\\User\\Desktop\\parcingProject\\parsed_articles\\buying-a-car-with-bitcoin-gets-37m-fine-prison-time-in-morocco-3079992.tar.xz\n",
      "Proceed article: https://www.investing.com/news/cryptocurrency-news/shariahcompliant-coins-release-set-to-spur-crypto-adoption-in-muslim-nations-3079945\n",
      "shariahcompliant-coins-release-set-to-spur-crypto-adoption-in-muslim-nations-3079945\n",
      "C:\\Users\\User\\Desktop\\parcingProject\\parsed_articles\\shariahcompliant-coins-release-set-to-spur-crypto-adoption-in-muslim-nations-3079945.tar.xz\n",
      "Proceed article: https://www.investing.com/news/cryptocurrency-news/binance-looks-to-the-uk-for-regulation-amid-us-crypto-crackdown-3079944\n",
      "binance-looks-to-the-uk-for-regulation-amid-us-crypto-crackdown-3079944\n",
      "C:\\Users\\User\\Desktop\\parcingProject\\parsed_articles\\binance-looks-to-the-uk-for-regulation-amid-us-crypto-crackdown-3079944.tar.xz\n",
      "Proceed article: https://www.investing.com/news/cryptocurrency-news/can-artificial-intelligence-create-more-jobs-3079916\n",
      "can-artificial-intelligence-create-more-jobs-3079916\n",
      "C:\\Users\\User\\Desktop\\parcingProject\\parsed_articles\\can-artificial-intelligence-create-more-jobs-3079916.tar.xz\n",
      "Proceed article: https://www.investing.com/news/cryptocurrency-news/crypto-market-will-recover-from-recent-panic-selling-says-analytics-platform-3079722\n",
      "crypto-market-will-recover-from-recent-panic-selling-says-analytics-platform-3079722\n",
      "C:\\Users\\User\\Desktop\\parcingProject\\parsed_articles\\crypto-market-will-recover-from-recent-panic-selling-says-analytics-platform-3079722.tar.xz\n",
      "Proceed article: https://www.investing.com/news/cryptocurrency-news/ny-authorizes-stablecoins-as-a-form-of-bail-assembly-bill-7024-3079676\n",
      "ny-authorizes-stablecoins-as-a-form-of-bail-assembly-bill-7024-3079676\n",
      "C:\\Users\\User\\Desktop\\parcingProject\\parsed_articles\\ny-authorizes-stablecoins-as-a-form-of-bail-assembly-bill-7024-3079676.tar.xz\n",
      "Proceed article: https://www.investing.com/news/cryptocurrency-news/ltcs-bullish-momentum-falters-as-overbought-signals-emerge-3079636\n",
      "ltcs-bullish-momentum-falters-as-overbought-signals-emerge-3079636\n",
      "C:\\Users\\User\\Desktop\\parcingProject\\parsed_articles\\ltcs-bullish-momentum-falters-as-overbought-signals-emerge-3079636.tar.xz\n",
      "Proceed article: https://www.investing.com/news/cryptocurrency-news/we-cant-keep-putting-bandaids-on-the-current-system--okx-3079635\n",
      "we-cant-keep-putting-bandaids-on-the-current-system--okx-3079635\n",
      "C:\\Users\\User\\Desktop\\parcingProject\\parsed_articles\\we-cant-keep-putting-bandaids-on-the-current-system--okx-3079635.tar.xz\n",
      "Proceed article: https://www.investing.com/news/cryptocurrency-news/pepe-link-and-sui-may-lead-the-charge-in-the-next-altcoin-rally-3079628\n",
      "pepe-link-and-sui-may-lead-the-charge-in-the-next-altcoin-rally-3079628\n",
      "C:\\Users\\User\\Desktop\\parcingProject\\parsed_articles\\pepe-link-and-sui-may-lead-the-charge-in-the-next-altcoin-rally-3079628.tar.xz\n",
      "Proceed article: https://www.investing.com/news/cryptocurrency-news/sora-card-signups-now-live-sora-enables-the-fusion-of-tradfi-and-defi-3079602\n",
      "sora-card-signups-now-live-sora-enables-the-fusion-of-tradfi-and-defi-3079602\n",
      "C:\\Users\\User\\Desktop\\parcingProject\\parsed_articles\\sora-card-signups-now-live-sora-enables-the-fusion-of-tradfi-and-defi-3079602.tar.xz\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import cfscrape\n",
    "from bs4 import BeautifulSoup\n",
    "import feedparser\n",
    "import os\n",
    "import tarfile\n",
    "\n",
    "RSS_FILE_URL = 'https://www.investing.com/rss/news_301.rss'\n",
    "\n",
    "\n",
    "def get_links_from_rss_file(_link):\n",
    "    entries = feedparser.parse(_link).entries\n",
    "    rss_parsed_links = [entry.link for entry in entries]\n",
    "    return rss_parsed_links\n",
    "\n",
    "\n",
    "def create_soup(_url, scrapper):\n",
    "    response = scrapper.get(_url)\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    return soup\n",
    "\n",
    "\n",
    "def get_data(_article, scrapper):\n",
    "    soup = create_soup(_article, scrapper)\n",
    "    article_date = soup.findAll('div', class_='contentSectionDetails')[1].text.lstrip()\n",
    "    article_title = soup.find('h1').text\n",
    "    del_article_scripts = soup.findAll('script')\n",
    "    for script in del_article_scripts:\n",
    "        script.decompose()\n",
    "\n",
    "    del_article_div = soup.findAll('div', class_='relatedInstrumentsWrapper')\n",
    "    for div in del_article_div:\n",
    "        div.decompose()\n",
    "\n",
    "    article_text = ''.join(soup.find('div', class_=\"WYSIWYG articlePage\").find_all(text=True)).lstrip()\n",
    "    article_pic = soup.find('div', class_=\"WYSIWYG articlePage\").find('img').get('src')\n",
    "    article_data = [article_title, article_date, article_text, article_pic]\n",
    "    return article_data\n",
    "\n",
    "\n",
    "def archive_files(link, files):\n",
    "    slug = link[link.rfind(str('/')) + 1:]\n",
    "    archive_name = f'{os.getcwd()}\\\\{slug}.tar.xz'\n",
    "    print(slug)\n",
    "    print(archive_name)\n",
    "    with tarfile.open(archive_name, 'w:xz') as tar_obj:\n",
    "        for _file in files:\n",
    "            tar_obj.add(_file)\n",
    "\n",
    "\n",
    "def parsing():\n",
    "    scrapper = cfscrape.create_scraper()\n",
    "    if not os.path.exists('parsed_articles'):\n",
    "        os.mkdir('parsed_articles')\n",
    "    os.chdir('parsed_articles')\n",
    "    rss_links = get_links_from_rss_file(RSS_FILE_URL)\n",
    "\n",
    "    new_articles = []\n",
    "    if not os.path.exists('list_of_articles.txt'):\n",
    "        with open('list_of_articles.txt', 'w', encoding='utf-8') as file_check:\n",
    "            pass\n",
    "\n",
    "    with open('list_of_articles.txt', 'r+', encoding='utf-8') as file:\n",
    "        file_data = [line.strip() for line in file.readlines()]\n",
    "        for url in rss_links:\n",
    "            if url not in file_data:\n",
    "                file.write(f'{url}')\n",
    "                file.write('\\n')\n",
    "                new_articles.append(url)\n",
    "\n",
    "    print(f\"Count articles for parsing {len(new_articles)}\")\n",
    "    for article in new_articles:\n",
    "        print(f\"Proceed article: {article}\")\n",
    "        info = get_data(article, scrapper)\n",
    "        with open('article_data.txt', 'w', encoding='utf-8') as text_file, open('article_pic.jpg',\n",
    "                                                                                'wb') as img_file, open(\n",
    "            'article_html.html', 'w', encoding='utf-8') as html_file, open('meta.json', 'w') as json_file:\n",
    "            for data in info[:3]:\n",
    "                text_file.write(data)\n",
    "                text_file.write('\\n')\n",
    "\n",
    "            pic = scrapper.get(info[3])\n",
    "            img_file.write(pic.content)\n",
    "\n",
    "            article_html = scrapper.get(article).text\n",
    "            html_file.write(article_html)\n",
    "\n",
    "            metadata_tags = create_soup(article, scrapper).findAll('meta')\n",
    "            metadata = {tag.get('name') or tag.get('property') or tag.get('http-equiv'): tag.get('content') for tag in\n",
    "                        metadata_tags}\n",
    "            json.dump(metadata, json_file, indent=3)\n",
    "\n",
    "            files_for_archive = [text_file.name, img_file.name, html_file.name, json_file.name]\n",
    "\n",
    "            archive_files(article, files_for_archive)\n",
    "\n",
    "        for file in files_for_archive:\n",
    "            os.remove(file)\n",
    "\n",
    "\n",
    "parsing()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76363edd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
